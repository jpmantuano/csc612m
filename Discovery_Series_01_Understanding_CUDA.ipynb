{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "LTplKN2sM3T-",
        "wi3j6_5ZNaJP"
      ],
      "authorship_tag": "ABX9TyO9WdfNHY0hw1DO0DR/wkdN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpmantuano/csc612m/blob/main/Discovery_Series_01_Understanding_CUDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Joseph Paulo L Mantuano"
      ],
      "metadata": {
        "id": "iRaovukV2lIM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1: C/C++ program version"
      ],
      "metadata": {
        "id": "Rrxt5ahV4cD0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2wNQEeE1sFb"
      },
      "outputs": [],
      "source": [
        "%%writefile convolve.cpp\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <ctime>\n",
        "#include <cstdio>\n",
        "\n",
        "void convolve1D(const std::vector<float>& in, std::vector<float>& out) {\n",
        "    int n = in.size();\n",
        "    if (n < 3) {\n",
        "        out.clear();\n",
        "        return;\n",
        "    }\n",
        "    out.resize(n - 2);\n",
        "    for (int i = 0; i < n - 2; ++i) {\n",
        "        out[i] = (in[i] + in[i + 1] + in[i + 2]) / 3.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int arr[] = {\n",
        "      256,\n",
        "      1024,\n",
        "      16777216, //2^24\n",
        "      67108864, //2^26\n",
        "      268435456 //2^28\n",
        "    };\n",
        "\n",
        "    int length = sizeof(arr) / sizeof(arr[0]);\n",
        "\n",
        "    for (int j = 0; j < length; j++) {\n",
        "\n",
        "        std::vector<float> _in_(arr[j]);\n",
        "\n",
        "        for (int i = 0; i < arr[j]; ++i) {\n",
        "            _in_[i] = static_cast<float>(i + 1);\n",
        "        }\n",
        "        std::vector<float> _out_;\n",
        "\n",
        "      clock_t start, end;\n",
        "      double cpu_time_used;\n",
        "      double sum = 0.0;\n",
        "\n",
        "      for (int run = 0; run < 10; run++) {\n",
        "        start = clock();  // Start time\n",
        "        convolve1D(_in_, _out_);\n",
        "        end = clock(); // End time\n",
        "\n",
        "        cpu_time_used = ((double) (end - start)) / CLOCKS_PER_SEC;\n",
        "        sum += cpu_time_used;\n",
        "      }\n",
        "\n",
        "      double avg_time = sum / 10;\n",
        "      printf(\"Average execution time for %d inputs: %.6f seconds\\n\", arr[j], avg_time);\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "g++ -o convolve convolve.cpp"
      ],
      "metadata": {
        "id": "D6BZWpWT57Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "./convolve"
      ],
      "metadata": {
        "id": "XRZPmwjx69bT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "c50kgcWPx1V-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CPU execution time (at least ten runs): ###"
      ],
      "metadata": {
        "id": "f6bj6uA9xw_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execution time in milliseconds.  Multiple kernel run is based on average execution time."
      ],
      "metadata": {
        "id": "UeztIvgoxrTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Number of elements | Avg. execution time |\n",
        "| ------------------ | ------------------- |\n",
        "| 256                | 0.005 ms            |\n",
        "| 1024               | 0.017 ms            |\n",
        "| 2^24               | 191.308 ms          |\n",
        "| 2^26               | 843.958 ms          |\n",
        "| 2^28               | 3345.247 ms         |"
      ],
      "metadata": {
        "id": "zvKi09rDx_mH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2: CUDA program version using grid-stride loop without prefetch"
      ],
      "metadata": {
        "id": "nBbuHtKo7_wZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CUDA_convolve.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "\n",
        "__global__ void conv1d_kernel(const float* in, float* out, int n) {\n",
        "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "  for (int i = idx; i < n - 2; i += stride) {\n",
        "      out[i] = (in[i] + in[i + 1] + in[i + 2]) / 3.0f;\n",
        "  }\n",
        "}\n",
        "\n",
        "int main(int argc, char* argv[]) {\n",
        "    if (argc < 2) {\n",
        "        std::cerr << \"Usage: \" << argv[0] << \" <N>\" << std::endl;\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    int N = std::atoi(argv[1]);\n",
        "    if (N <= 0) {\n",
        "        std::cerr << \"N must be a positive integer.\" << std::endl;\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    // const int N = 256;\n",
        "\n",
        "    // Use vector for dynamic size and easy modification\n",
        "    std::vector<float> h_in(N);\n",
        "    std::vector<float> h_out(N, 0.0f);\n",
        "\n",
        "    // Initialize h_in with values 1.0, 2.0, 3.0, ..., N\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        h_in[i] = static_cast<float>(i + 1);\n",
        "    }\n",
        "\n",
        "    float *d_in = nullptr, *d_out = nullptr;\n",
        "    cudaMalloc(&d_in, N * sizeof(float));\n",
        "    cudaMalloc(&d_out, N * sizeof(float));\n",
        "\n",
        "    cudaMemcpy(d_in, h_in.data(), N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    int threadsPerBlock = 1024;\n",
        "    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;\n",
        "//    int threadsPerBlock = (N < 1024) ? N : 1024;\n",
        "//    int blocksPerGrid = 1;\n",
        "\n",
        "    conv1d_kernel<<<blocksPerGrid, threadsPerBlock>>>(d_in, d_out, N);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    cudaMemcpy(h_out.data(), d_out, N * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "//    std::cout << \"Convolution output:\\n\";\n",
        "//    for (int i = 0; i < N - 2; ++i) {\n",
        "//        std::cout << h_out[i] << \" \";\n",
        "//    }\n",
        "//    std::cout << std::endl;\n",
        "\n",
        "    cudaFree(d_in);\n",
        "    cudaFree(d_out);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "nMO62VMY8BJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvcc CUDA_convolve.cu -o CUDA_convolve -arch=sm_75"
      ],
      "metadata": {
        "id": "0lbGZX5HG-t-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "./CUDA_convolve 16777216"
      ],
      "metadata": {
        "id": "nBPUQsTZIYJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvprof --unified-memory-profiling per-process-device --print-gpu-trace ./CUDA_convolve 16777216"
      ],
      "metadata": {
        "id": "C-hcFO2zH6hP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvprof ./CUDA_convolve 16777216"
      ],
      "metadata": {
        "id": "83S7FwP-ZoQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "for i in {1..10}\n",
        "do\n",
        "  nvprof ./CUDA_convolve 16777216 2>&1 | tee run_$i.log\n",
        "done"
      ],
      "metadata": {
        "id": "0vvVFsl-S7wD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execution time in milliseconds.  Multiple kernel run is based on average execution time. ###"
      ],
      "metadata": {
        "id": "eVS5LmQxyOBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a.) Number of blocks/grid = 1\n",
        "\n",
        "| Block size (2^24 elements) | Single kernel run | Multiple kernel run |\n",
        "| -------------------------- | ----------------- | ------------------- |\n",
        "| Block size = 1024          | 12.633ms          | 12.6288ms           |"
      ],
      "metadata": {
        "id": "doyVVpRsyOLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "b.) Number of blocks/grid = max (as per formula)\n",
        "\n",
        "| Block size (2^24 elements) | Single kernel run | multiple kernel run |\n",
        "| -------------------------- | ----------------- | ------------------- |\n",
        "| Block size = 1024          | 1.1660 ms         | 1.1668 ms           |"
      ],
      "metadata": {
        "id": "6X5Sygt4yONc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "c.) Unified memory profile - data transfer (2^24 elements)\n",
        "\n",
        "| Type           | Total size | Total time |\n",
        "| -------------- | ---------- | ---------- |\n",
        "| host to device | 64 MB      | 14.217ms   |\n",
        "| device to host | 64 MB      | 14.899ms   |"
      ],
      "metadata": {
        "id": "x2qZ-pxTyOP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "d.) Unified memory profile - page fault (2^24 elements)\n",
        "\n",
        "| Type                 | Count | Total time |\n",
        "| -------------------- | ----- | ---------- |\n",
        "| GPU page fault group |       |            |\n",
        "| CPU Page fault group |       | -----      |"
      ],
      "metadata": {
        "id": "pp76TLJKyOSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a.) What is the effect of having one block only vs. max block in terms of execution time?  Why is it so?\n",
        "- The number of blocks allows the increase parallelism and utilization of GPU resources. Having one block, all executions are processed in a single GPU processor while having max blocks allows the exploit of maximizing GPU processors.\n",
        "\n",
        "b.) What is the effect of single run vs multiple run in terms of execution time? Why is it so?\n",
        "- The average execution time decreases if taken after multiple runs vs a single run. Because in a multiple run there is a cache effect after transferring the data after the first run making the succeeding runs faster because of lower overhead compared to a single run.\n",
        "\n",
        "c.) In the unified memory profile, is the data transfer time included in the GPU execution time?  Explain your answer.\n",
        "- the data transfer time is generally included in the GPU execution time reported by the profiler. The amount of time taken to move data from host to device and device to host for processing by the GPU.\n",
        "\n",
        "d.)  In the unified memory profile, is the page fault time included in the GPU execution time?  Explain your answer.\n",
        "- No, the code uses cudaMalloc which does explicit memory allocations and not unified memory.\n",
        "\n",
        "e.) What is the speedup (or speed down) of the execution time of GPU (including all the overhead) compare to C/C++? Is GPU execution time better or worse?\n",
        "- In general the GPU execution is faster, even on the one block runs. But when running with smaller number of elements for computation the difference between GPU runtime vs CPU runtime is smaller. Probably due to the amount of processing needed with smaller data, the advantage of using GPU for processing decreases."
      ],
      "metadata": {
        "id": "zosA9xb2ymDU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3: CUDA Program Version Using Grid-Stride Loop With Prefetching"
      ],
      "metadata": {
        "id": "LTplKN2sM3T-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CUDA_convolve_prefetch.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <iostream>\n",
        "\n",
        "__global__ void conv1d_kernel(const float* __restrict__ in, float* __restrict__ out, int n) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "    for (int i = idx; i < n - 2; i += stride) {\n",
        "        out[i] = (in[i] + in[i + 1] + in[i + 2]) / 3.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char* argv[]) {\n",
        "    if (argc < 2) {\n",
        "        std::cerr << \"Usage: \" << argv[0] << \" <N>\" << std::endl;\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    int N = std::atoi(argv[1]);\n",
        "    if (N <= 0) {\n",
        "        std::cerr << \"N must be a positive integer.\" << std::endl;\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    // const int N = 256;\n",
        "\n",
        "    float *h_in = nullptr;\n",
        "    float *h_out = nullptr;\n",
        "\n",
        "    // Allocate unified memory (managed memory)\n",
        "    cudaMallocManaged(&h_in, N * sizeof(float));\n",
        "    cudaMallocManaged(&h_out, N * sizeof(float));\n",
        "\n",
        "    // Initialize input\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        h_in[i] = float(i + 1);\n",
        "        h_out[i] = 0.0f;\n",
        "    }\n",
        "\n",
        "    // Prefetch input and output to GPU\n",
        "    int device = -1;\n",
        "    cudaGetDevice(&device);\n",
        "    cudaMemPrefetchAsync(h_in, N * sizeof(float), device);\n",
        "    cudaMemPrefetchAsync(h_out, N * sizeof(float), device);\n",
        "\n",
        "    int threadsPerBlock = 1024;\n",
        "    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;\n",
        "//    int threadsPerBlock = (N < 1024) ? N : 1024;\n",
        "//    int blocksPerGrid = 1;\n",
        "\n",
        "    conv1d_kernel<<<blocksPerGrid, threadsPerBlock>>>(h_in, h_out, N);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Prefetch output back to CPU (optional)\n",
        "    cudaMemPrefetchAsync(h_out, N * sizeof(float), cudaCpuDeviceId);\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "//    std::cout << \"Convolution output:\\n\";\n",
        "//    for (int i = 0; i < N - 2; ++i) {\n",
        "//        std::cout << h_out[i] << \" \";\n",
        "//    }\n",
        "//    std::cout << std::endl;\n",
        "\n",
        "    cudaFree(h_in);\n",
        "    cudaFree(h_out);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "rBdYxPsHM4PN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvcc CUDA_convolve_prefetch.cu -o CUDA_convolve_prefetch -arch=sm_75"
      ],
      "metadata": {
        "id": "nYzY4bhuNGzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "./CUDA_convolve_prefetch 16777216"
      ],
      "metadata": {
        "id": "NkBwBp99NG15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvprof --unified-memory-profiling per-process-device --print-gpu-trace ./CUDA_convolve_prefetch 16777216"
      ],
      "metadata": {
        "id": "XNC33JJLcZBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvprof ./CUDA_convolve_prefetch 16777216"
      ],
      "metadata": {
        "id": "LH_CbXZqNG4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "for i in {1..10}\n",
        "do\n",
        "  nvprof ./CUDA_convolve_prefetch 16777216 2>&1 | tee run_$i.log\n",
        "done"
      ],
      "metadata": {
        "id": "h8S_6xp_cfUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execution time in milliseconds.  Multiple kernel run is based on average execution time. ###"
      ],
      "metadata": {
        "id": "9pJYkP1KywAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a.) Number of blocks/grid = max (as per formula)\n",
        "\n",
        "| Block size (2^24 elements) | Single kernel run | multiple kernel |\n",
        "| -------------------------- | ----------------- | --------------- |\n",
        "| Block size = 1024          | 1.1652ms          | 1.16742ms       |"
      ],
      "metadata": {
        "id": "F4h9in4_ywCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "b.)Unified memory profile - data transfer (2^24 elements)\n",
        "\n",
        "| Type           | Total size | Total time |\n",
        "| -------------- | ---------- | ---------- |\n",
        "| host to device | 128 MB     | 11.13964ms |\n",
        "| device to host | 64 MB      | 5.142931ms |"
      ],
      "metadata": {
        "id": "nNL0sMNhywE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "c.) Unified memory profile - page fault (2^24 elements)\n",
        "\n",
        "| Type                 | Count | Total time |\n",
        "| -------------------- | ----- | ---------- |\n",
        "| GPU page fault group |       |            |\n",
        "| CPU Page fault group | 384   | ---------  |"
      ],
      "metadata": {
        "id": "RjLXcFuXywHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a.) In the unified memory profile, is the data transfer time included in the GPU execution time?  Explain your answer.\n",
        "- the data transfer time is generally included in the GPU execution time reported by the profiler. The amount of time taken to move data from host to device and device to host for processing by the GPU.\n",
        "\n",
        "b.) In the unified memory profile, is the page fault time included in the GPU execution time?  Explain your answer.\n",
        "- Yes, page faults is included in the profile results. Using cudaMallocManaged, which makes a unified address for both CPU and GPU, when the GPU accesses a memory page not currently resident in its local memory, a page fault occurs.\n",
        "\n",
        "c.) What is the speedup (or speed down) of the execution time of GPU with prefetching compare to without prefetching?  Is GPU execution time (with prefetching) better or worse? Include all the overhead in the computation.\n",
        "- There is a speed up around ~ 12ms with prefetching. Probably due to the data transfer not blocking the kernel processing time, since the data was already transfer prior to processing.\n",
        "\n",
        "d.) For this case, is there an effect in execution time if kernel is executed multiple times as compare to executing once only? Why is it so?\n",
        "- The runs shows very small difference in runtimes. Probably because if using prefetching, data is already moved to the GPU, it reduces the overall impact in processing / kernel execution."
      ],
      "metadata": {
        "id": "psnvJkV2ywL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4: CUDA Program Version Using Grid-Stride Loop With Prefetch and Memory Advice"
      ],
      "metadata": {
        "id": "wi3j6_5ZNaJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CUDA_convolve_prefetch_mem_advice.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <iostream>\n",
        "\n",
        "__global__ void conv1d_kernel(const float* __restrict__ in, float* __restrict__ out, int n) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "    for (int i = idx; i < n - 2; i += stride) {\n",
        "        out[i] = (in[i] + in[i + 1] + in[i + 2]) / 3.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char* argv[]) {\n",
        "    if (argc < 2) {\n",
        "        std::cerr << \"Usage: \" << argv[0] << \" <N>\" << std::endl;\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    int N = std::atoi(argv[1]);\n",
        "    if (N <= 0) {\n",
        "        std::cerr << \"N must be a positive integer.\" << std::endl;\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    //const int N = 256;\n",
        "\n",
        "    float *h_in = nullptr;\n",
        "    float *h_out = nullptr;\n",
        "\n",
        "    // Allocate unified memory\n",
        "    cudaMallocManaged(&h_in, N * sizeof(float));\n",
        "    cudaMallocManaged(&h_out, N * sizeof(float));\n",
        "\n",
        "    // Initialize input\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        h_in[i] = float(i + 1);\n",
        "        h_out[i] = 0.0f;\n",
        "    }\n",
        "\n",
        "    int device = -1;\n",
        "    cudaGetDevice(&device);\n",
        "\n",
        "    // Advise that h_in will be mostly read on device\n",
        "    cudaMemAdvise(h_in, N * sizeof(float), cudaMemAdviseSetReadMostly, device);\n",
        "    // Advise that h_out will be mostly written on device\n",
        "    cudaMemAdvise(h_out, N * sizeof(float), cudaMemAdviseSetPreferredLocation, device);\n",
        "\n",
        "    // Prefetch to device\n",
        "    cudaMemPrefetchAsync(h_in, N * sizeof(float), device);\n",
        "    cudaMemPrefetchAsync(h_out, N * sizeof(float), device);\n",
        "\n",
        "    int threadsPerBlock = 1024;\n",
        "    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;\n",
        "//    int threadsPerBlock = (N < 1024) ? N : 1024;\n",
        "//    int blocksPerGrid = 1;\n",
        "\n",
        "    conv1d_kernel<<<blocksPerGrid, threadsPerBlock>>>(h_in, h_out, N);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Prefetch output back to host\n",
        "    cudaMemPrefetchAsync(h_out, N * sizeof(float), cudaCpuDeviceId);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "//    std::cout << \"Convolution output:\\n\";\n",
        "//    for (int i = 0; i < N - 2; ++i) {\n",
        "//        std::cout << h_out[i] << \" \";\n",
        "//    }\n",
        "//    std::cout << std::endl;\n",
        "\n",
        "    cudaFree(h_in);\n",
        "    cudaFree(h_out);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "fhpD88OJNdHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvcc CUDA_convolve_prefetch_mem_advice.cu -o CUDA_convolve_prefetch_mem_advice -arch=sm_75"
      ],
      "metadata": {
        "id": "ZBUKM-DBNdJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "./CUDA_convolve_prefetch_mem_advice 256"
      ],
      "metadata": {
        "id": "K9q_fWXENdMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvprof --unified-memory-profiling per-process-device --print-gpu-trace ./CUDA_convolve_prefetch_mem_advice 16777216"
      ],
      "metadata": {
        "id": "iY7pYSOfc4NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvprof ./CUDA_convolve_prefetch_mem_advice 268435456"
      ],
      "metadata": {
        "id": "NFzS3YpZNdOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "for i in {1..10}\n",
        "do\n",
        "  nvprof ./CUDA_convolve_prefetch_mem_advice 268435456 2>&1 | tee run_$i.log\n",
        "done"
      ],
      "metadata": {
        "id": "3SzDXOridAMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execution time in milliseconds.  Multiple kernel run is based on average execution time. ###"
      ],
      "metadata": {
        "id": "x5EX5dx0zJOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a.) Number of blocks/grid = max (as per formula)\n",
        "\n",
        "| Block size (2^24 elements) | Single kernel run | multiple kernel |\n",
        "| -------------------------- | ----------------- | --------------- |\n",
        "| Block size = 1024          | 1.1690ms          | 1.16706ms       |"
      ],
      "metadata": {
        "id": "GGzUegsfzJQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "b.)Unified memory profile - data transfer (2^24 elements)\n",
        "\n",
        "| Type           | Total size | Total time |\n",
        "| -------------- | ---------- | ---------- |\n",
        "| host to device | 128 MB     | 11.14467ms |\n",
        "| device to host | 64 MB      | 5.151089ms |"
      ],
      "metadata": {
        "id": "k3y3j7GnzJTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "c.) Unified memory profile - page fault (2^24 elements)\n",
        "\n",
        "| Type                 | Count | Total time |\n",
        "| -------------------- | ----- | ---------- |\n",
        "| GPU page fault group |       |            |\n",
        "| CPU Page fault group | 384   | --------   |"
      ],
      "metadata": {
        "id": "tAFZHWBkzJVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a.) In the unified memory profile, is the data transfer time included in the GPU execution time?  Explain your answer.\n",
        "\n",
        "b.) What is the speedup (or speed down) of the execution time of GPU (with prefetching, page creation and memadvise) compare to execution time of C?  Is GPU execution time better or worse? Include all the overhead in the computation.\n",
        "\n",
        "c.) For this case, is there an effect in execution time if kernel is executed multiple times as compare to executing once only? Why is it so?"
      ],
      "metadata": {
        "id": "bUMIcECCzZuk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execution time in milliseconds.  Multiple kernel run is based on average execution time ###"
      ],
      "metadata": {
        "id": "YN2-xLihzdyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grid-stride loop (prefetching with \"page creation\" and mem advise)"
      ],
      "metadata": {
        "id": "X6LAPlcAzg0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### block size = 256 threads ###"
      ],
      "metadata": {
        "id": "CyYVxrDvzy4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| # of elements | Average based on multiple  kernel run | Speedup compare to C | Speedup compare to single run (no prefetch) |\n",
        "| ------------- | ------------------------------------- | -------------------- | ------------------------------------------- |\n",
        "| 256           | 88.448 us                             | 0.005 ms             | 7.264 us                                    |\n",
        "| 1024          | 12.608 us                             | 0.017 ms             | 7.456 us                                    |\n",
        "| 2^24          | 17.168322 ms                          | 191.308 ms           | 30.16546 ms                                 |\n",
        "| 2^26          | 68.26708 ms                           | 843.958 ms           | 118.8253 ms                                 |\n",
        "| 2^28          | 269.20204 ms                          | 3345.247 ms          | 474.072 ms                                  |"
      ],
      "metadata": {
        "id": "SvCCD98Xzlmf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2^24 [16777216] elements #####"
      ],
      "metadata": {
        "id": "cQypuIYVz6Yd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Block size (2^24 elements) | Single kernel run | multiple kernel run |\n",
        "| -------------------------- | ----------------- | ------------------- |\n",
        "| 256                        | 17.161082 ms      | 17.166806 ms        |\n",
        "| 512                        | 17.157936 ms      | 17.155219 ms        |\n",
        "| 1024                       | 17.168466 ms      | 17.156158 ms        |"
      ],
      "metadata": {
        "id": "WRGMhupwzloW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2^26 [67108864] elements #####"
      ],
      "metadata": {
        "id": "E4cArceJ0EnD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Block size (2^26 elements) | Single kernel run | multiple kernel run |\n",
        "| -------------------------- | ----------------- | ------------------- |\n",
        "| 256                        | 68.67913 ms       | 67.85576 ms         |\n",
        "| 512                        | 69.0517 ms        | 68.09373 ms         |\n",
        "| 1024                       | 69.22053 ms       | 67.18557 ms         |"
      ],
      "metadata": {
        "id": "vuQVgmnmzlq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2^28 [268435456] elements #####"
      ],
      "metadata": {
        "id": "X4aSV0E60MMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Block size (2^28 elements) | Single kernel run | multiple kernel run |\n",
        "| -------------------------- | ----------------- | ------------------- |\n",
        "| 256                        | 275.75955 ms      | 273.18701 ms        |\n",
        "| 512                        | 270.15548 ms      | 271.33376 ms        |\n",
        "| 1024                       | 275.72904 ms      | 269.23974 ms        |"
      ],
      "metadata": {
        "id": "itFpJFX6zltQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a.) What is the effect of the number of elements in the execution time?  What is the rate of increase (i.e., linear, logarithmic, exponential, etc.)?\n",
        "- the number of elements increases the runtime. Across all the runs, the increase in runtimes are mostly linear with outlier in some occasions. The kernel session was restarted every set of runs to ensure a clean environment.\n",
        "\n",
        "b.) How does block size affect execution time (observing various elements and using max blocks)?  Which block size will you recommend?\n",
        "- there is a slight improvements in runtime as the block size is being increased, with outliers in the single kernel run. But overall, the improvement in runtime for multiple kernel runs seems to improve or is within range of the runtime from the other block sizes. This might be due to the size of the elements being tested. More tests with a larger number of elements can prove if the outliers is due to a small number of elements during the test.\n",
        "\n",
        "c.) Is prefetching always recommended?  Can you think of a situation in which no prefetching is better?\n",
        "- In general prefetching is recommended, because it can help to reduce processing overhead, by moving data to the GPU prior to being accessed. But similar to caching it may sometimes bring unnecessary data transfer to the GPU causing traffic and not bringing performance gain. Or when the data is small enough and processing time is short enough, the overhead of prefetching does not bring any significant gain."
      ],
      "metadata": {
        "id": "jP0vzYj-zlvz"
      }
    }
  ]
}